{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = path_home + \"spark-3.3.2-bin-hadoop3\"\n",
    "os.environ[\"JAVA_HOME\"] = path_home + \"jre1.8.0_361\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run this after findspark.init()\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('diabetes-test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_cases = spark.read.load(path_home + \"data/kaggle_diabetes_dataset/diabetes.csv\",\n",
    "                        format=\"csv\", \n",
    "                        sep=\",\", \n",
    "                        inferSchema=\"true\", \n",
    "                        header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_cases.limit(10).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple pyspark RDD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "diabetes_cases = diabetes_cases.toDF(*['times_pregnancies', 'glucose_concentration', 'diastolic_bloodpressure',\n",
    "                                        'skin_thickness', 'insulin_rate', 'bmi', 'diabetes_pedigree', 'age', 'outcome'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descending Sort\n",
    "from pyspark.sql import functions as F\n",
    "diabetes_cases.sort(F.desc(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get old people with normal glucose concentration\n",
    "diabetes_cases.filter((diabetes_cases.age>60) & (diabetes_cases.glucose_concentration>=70) & (diabetes_cases.glucose_concentration<=100)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby\n",
    "from pyspark.sql import functions as F\n",
    "diabetes_cases.groupBy([\"Age\"]).agg(F.sum(\"times_pregnancies\")).sort(F.desc(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestamp = spark.sql(\"\"\"\n",
    "  SELECT TO_DATE(CAST(UNIX_TIMESTAMP() AS TIMESTAMP)) AS newdate\"\"\"\n",
    ")\n",
    "\n",
    "datestamp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Get Year from date in pyspark\n",
    " \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df1_test = diabetes_cases.withColumn(\"year\", current_date())\n",
    "df1_test.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed above experimental code into pragmatic ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_up = True\n",
    "\n",
    "def spark_session():\n",
    "    return SparkSession.builder.appName('diabetes-spark-processor').getOrCreate()\n",
    "\n",
    "def extract_dataset(dataset) -> object:\n",
    "    if os.environ['SPARK_HOME']:\n",
    "        try:\n",
    "            return spark.read.load(dataset,\n",
    "                            format=\"csv\", \n",
    "                            sep=\",\", \n",
    "                            inferSchema=\"true\", \n",
    "                            header=\"true\")\n",
    "        except Exception as err:\n",
    "            logging.fatal(err)\n",
    "\n",
    "def rename_columns(dataset):\n",
    "    return dataset.toDF(*['times_pregnancies', 'glucose_concentration', 'diastolic_bloodpressure',\n",
    "                                        'skin_thickness', 'insulin_rate', 'bmi', 'diabetes_pedigree', 'age', 'outcome'])\n",
    "\n",
    "def add_date(dataset):\n",
    "    return diabetes_cases.withColumn(\"date\", current_date())\n",
    "\n",
    "def transform_dataset(dataset):\n",
    "    if cluster_up:\n",
    "        return rename_columns(dataset), add_date(dataset)\n",
    "    \n",
    "def load_dataset(dataset):\n",
    "    import boto3_mocking\n",
    "    with boto3_mocking.resources.handler_for('s3'):\n",
    "        return dataset.write.parquet()(\"s3a://sparkbyexamples/parquet/people.parquet\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
